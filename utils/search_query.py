import requests
import trafilatura
import google.generativeai as genai
from dotenv import load_dotenv
import os
import time
from unidecode import unidecode
import requests
from bs4 import BeautifulSoup
import urllib.parse
import sys
from config import Config

class WebSraper:
    def __init__(self, num_results=5):
        self.num_results = num_results
        self.config = Config()

    def search_serper(self, query, max_retries=1):
        url = "https://google.serper.dev/search"
        headers = self.config.get_serper_api_headers()
        data = {"q": query}
        
        for attempt in range(max_retries):
            try:
                res = requests.post(url, headers=headers, json=data)
                res.raise_for_status()
                results = res.json()
                if "organic" in results and results["organic"]:
                    return [{"url": r["link"], "title": r["title"]} for r in results["organic"][:num_results]]
                else:
                    print(f"âš ï¸ No organic results on attempt {attempt+1}. Retrying...")
            except Exception as e:
                print(f"âŒ Error during search (attempt {attempt+1}): {e}")
            time.sleep(1)
        
        return []
    
    def search_duckduckgo(self, query):
        params = {'q': query}
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get('https://html.duckduckgo.com/html/', params=params, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        results = soup.find_all('a', class_='result__a')
        
        entries = []
        cnt = 0

        for link in results:
            if cnt >= 5:
                break
            cnt += 1
            raw_url = link['href']
            parsed = urllib.parse.urlparse(raw_url)
            query_params = urllib.parse.parse_qs(parsed.query)
            decoded_url = query_params.get("uddg", [raw_url])[0]
            entry = {
                "title": link.text.strip(),
                "url": decoded_url
            }
            entries.append(entry)

        return entries
    
    def search_openai(self, query):
        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o chÄƒm sÃ³c khÃ¡ch hÃ ng.

        DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¢u há»i tá»« khÃ¡ch hÃ ng:
        {query}
        YÃªu cáº§u cá»§a báº¡n lÃ  tÃ¬m kiáº¿m thÃ´ng tin trÃªn Internet vÃ  tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, Ä‘Ãºng trá»ng tÃ¢m.
        HÃ£y cung cáº¥p cáº£ liÃªn káº¿t Ä‘áº¿n cÃ¡c nguá»“n thÃ´ng tin báº¡n sá»­ dá»¥ng Ä‘á»ƒ tráº£ lá»i.
        Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan, hÃ£y tráº£ lá»i: "TÃ´i xin lá»—i, hiá»‡n táº¡i tÃ´i chÆ°a tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u. Báº¡n vui lÃ²ng liÃªn há»‡ chuyÃªn viÃªn Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ thÃªm."

        """
        client = self.config.get_openai_api_client()
        completion = client.chat.completions.create(
            model="gpt-4o-search-preview",
            web_search_options={
                "user_location": {
                    "type": "approximate",
                    "approximate": {
                        "country": "VN",
                        "city": "Ho Chi Minh City",      
                        "region": "Ho Chi Minh City",    
                    }
                },
            },
            messages=[{
                "role": "user",
                "content": prompt
            }],
        )
        
        return completion.choices[0].message.content

class ArticleExtractor:
    def extract_article_with_source(self, entry):
        url = entry["url"]
        title = entry["title"]
        downloaded = trafilatura.fetch_url(url)
        content = trafilatura.extract(downloaded)
        if content:
            return {
                "title": title,
                "url": url,
                "content": content
            }
        return None

class Summarizer:
    def __init__(self):
        self.config = Config()
        self.model = self.config.get_gemini_api_model()

    def summarize_articles(self, articles, query):
        combined = ""
        for art in articles:
            combined += f"""TiÃªu Ä‘á»: {art['title']}
    URL: {art['url']}

    {art['content']}

    """

        prompt = f"""
    Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o chÄƒm sÃ³c khÃ¡ch hÃ ng, chuyÃªn tráº£ lá»i cÃ¡c cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ká»¹ thuáº­t hoáº·c quy Ä‘á»‹nh phÃ¡p luáº­t Ä‘Æ°á»£c cung cáº¥p.

    DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ tÃ i liá»‡u tham kháº£o Ä‘Æ°á»£c thu tháº­p tá»« cÃ¡c nguá»“n uy tÃ­n trÃªn Internet:

    {combined}

    CÃ¢u há»i cá»§a khÃ¡ch hÃ ng:
    {query}

    YÃªu cáº§u Ä‘á»‘i vá»›i cÃ¢u tráº£ lá»i:
    1. Tráº£ lá»i trá»±c tiáº¿p vÃ  chÃ­nh xÃ¡c cÃ¢u há»i cá»§a khÃ¡ch hÃ ng báº±ng tiáº¿ng Viá»‡t dá»… hiá»ƒu.
    2. Giáº£i thÃ­ch lÃ½ do vÃ¬ sao báº¡n Ä‘Æ°a ra cÃ¢u tráº£ lá»i Ä‘Ã³, dá»±a trÃªn ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p. KhÃ´ng sá»­ dá»¥ng cÃ¡ch Ä‘Ã¡nh sá»‘ nhÆ° "Äoáº¡n 1", "Äoáº¡n 2" v.v.
    3. TrÃ­ch dáº«n láº¡i thÃ´ng tin cÃ³ liÃªn quan (bao gá»“m ná»™i dung vÃ  URL) Ä‘á»ƒ chá»©ng minh cho cÃ¢u tráº£ lá»i.
    4. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p, hÃ£y tráº£ lá»i: "TÃ´i xin lá»—i, hiá»‡n táº¡i tÃ´i chÆ°a tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u. Báº¡n vui lÃ²ng liÃªn há»‡ chuyÃªn viÃªn Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ thÃªm."
    5. Náº¿u khÃ¡ch hÃ ng yÃªu cáº§u gáº·p chuyÃªn viÃªn (vÃ­ dá»¥: "TÃ´i muá»‘n gáº·p chuyÃªn viÃªn"), hÃ£y tráº£ lá»i: "TÃ´i sáº½ gá»­i yÃªu cáº§u cá»§a báº¡n Ä‘áº¿n chuyÃªn viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng. Há» sáº½ liÃªn há»‡ vá»›i báº¡n trong thá»i gian sá»›m nháº¥t."

    LuÃ´n giá»¯ thÃ¡i Ä‘á»™ lá»‹ch sá»±, chuyÃªn nghiá»‡p vÃ  tráº£ lá»i rÃµ rÃ ng, máº¡ch láº¡c.
    """
        
        response = self.model.generate_content(prompt)
        return response.text

class QueryProcessor:
    def __init__(self, searcher=WebSraper(), extractor=ArticleExtractor(), summarizer=Summarizer()):
        self.searcher = searcher
        self.extractor = extractor
        self.summarizer = summarizer

    def remove_accents(self, text):
        return unidecode(text)

    def search_and_answer(self, query, search_engine="openai"):
        print("ğŸ” Searching:", query)
        results = None

        if search_engine == "openai":
            print("Search with OpenAI...")
            results = self.searcher.search_openai(query)
            return results
        elif search_engine == "serper":
            print("Search with Serper...")
            results = self.searcher.search_serper(query)

            if not results:
                query_no_accents = self.remove_accents(query)
                print("ğŸ” Retry with non-accented query:", query_no_accents)
                results = self.searcher.search_serper(query_no_accents)
        elif search_engine == "duckduckgo":
            print("Search with DuckDuckGo...")
            results = self.searcher.search_duckduckgo(query)

        if not results:
            return "âš ï¸ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ tÃ¬m kiáº¿m phÃ¹ há»£p."

        print("ğŸ”— Found URLs:")
        for r in results:
            print(f"{r['title']} - {r['url']}")

        articles = [self.extractor.extract_article_with_source(r) for r in results]
        articles = [a for a in articles if a and a["content"]]

        if not articles:
            return "âš ï¸ KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c bÃ i viáº¿t."

        summary = self.summarizer.summarize_articles(articles, query)
        return summary
