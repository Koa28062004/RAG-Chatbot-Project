import re
import difflib
import time
import unicodedata
import os
import json
import numpy as np

from collections import defaultdict
from typing import Dict, Tuple, List
from config import Config
from utils.search_query import QueryProcessor
from utils.embedding import BM25EmbeddingFunction


class AnswerGenerator:
    def __init__(self):
        self.config = Config()
        self.model = self.config.get_model_used()
        self.query_processor = QueryProcessor()
        self.FALLBACK_TRIGGERS = [
            "t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin",
            "t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin",
            "r·∫•t ti·∫øc, hi·ªán t·∫°i t√¥i ch∆∞a",
            "b·∫°n vui l√≤ng li√™n h·ªá chuy√™n vi√™n",
            "t√¥i xin l·ªói",
            "vui l√≤ng li√™n h·ªá chuy√™n vi√™n"
        ]

    def generate_openai_answer(self, prompt: str) -> str:
        start = time.time()
        response = self.model.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2048
        )
        print(f"‚è±Ô∏è OpenAI generation time: {time.time() - start:.2f}s")
        return response.choices[0].message.content.strip()

    def generate_gemini_answer(self, prompt: str):
        response = self.model.generate_content(prompt)
        return response.text

    def generate_model_answer(self, prompt: str) -> str:
        if self.config.MODEL_USED == "gemini":
            return self.generate_gemini_answer(prompt)
        elif self.config.MODEL_USED == "openai":
            return self.generate_openai_answer(prompt)
        else:
            raise ValueError(f"Unsupported model: {self.config.MODEL_USED}")

    def normalize_viet(self, query: str) -> str:
        prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω chuy√™n x·ª≠ l√Ω ng√¥n ng·ªØ chuy√™n ng√†nh ph√≤ng ch√°y ch·ªØa ch√°y (PCCC).
        H√£y vi·∫øt l·∫°i truy v·∫•n d∆∞·ªõi ƒë√¢y th√†nh m·ªôt c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, r√µ r√†ng, s·ª≠ d·ª•ng ng√¥n ng·ªØ k·ªπ thu·∫≠t chu·∫©n nh∆∞ trong c√°c t√†i li·ªáu QCVN ho·∫∑c TCVN.
        M·ª•c ti√™u:
        - Gi·ªØ nguy√™n t·ª´ kh√≥a ch√≠nh v√† n·ªôi dung g·ªëc c·ªßa truy v·∫•n.
        - B·ªï sung ƒë·∫ßy ƒë·ªß b·ªëi c·∫£nh, t√¨nh hu·ªëng ho·∫∑c ƒëi·ªÅu ki·ªán n·∫øu c√≥ th·ªÉ, ƒë·ªÉ gi√∫p truy v·∫•n d·ªÖ ƒë∆∞·ª£c t√¨m th·∫•y trong t√†i li·ªáu k·ªπ thu·∫≠t.
        - Tr√°nh vi·∫øt t·∫Øt, t·ª´ l√≥ng ho·∫∑c ng√¥n ng·ªØ ƒë·ªãa ph∆∞∆°ng.
        Ch·ªâ tr·∫£ v·ªÅ truy v·∫•n ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i, kh√¥ng c·∫ßn gi·∫£i th√≠ch.

        Truy v·∫•n g·ªëc: "{query}"

        Truy v·∫•n ƒë√£ m·ªü r·ªông v√† chu·∫©n h√≥a:
        """
        response = self.generate_model_answer(prompt)
        return response

    def normalize_eng(self, query: str) -> str:
        prompt = f"""
        You are a professional assistant specializing in fire prevention and fighting (PCCC) technical language.
        Rewrite the following query clearly, avoiding abbreviations or local dialects.
        The goal is to make this query easily understandable and searchable in standard technical documents like QCVN, TCVN.
        Only return the standardized question, no additional explanations.

        Original query: "{query}"

        Standardized query (using clear technical language):
        """
        response = self.generate_model_answer(prompt)
        return response

    def make_rag_prompt_viet(self, query: str, context_block: str) -> str:
        prompt = f"""
    B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o chuy√™n nghi·ªáp, c√≥ nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng d·ª±a tr√™n n·ªôi dung t√†i li·ªáu k·ªπ thu·∫≠t ho·∫∑c vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.

    **H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:**
    1. **Ph√¢n t√≠ch k·ªπ c√¢u h·ªèi v√† x√°c ƒë·ªãnh c√°c t·ª´ kh√≥a quan tr·ªçng.** So s√°nh tr·ª±c ti·∫øp c√°c t·ª´ kh√≥a n√†y v·ªõi ti√™u ƒë·ªÅ ph·∫ßn/ch∆∞∆°ng trong t√†i li·ªáu ƒë·ªÉ t√¨m ph·∫ßn ph√π h·ª£p nh·∫•t. V√≠ d·ª•, **h√∫t kh√≥i** kh√°c v·ªõi **·ªëng th√¥ng gi√≥** ‚Äî c·∫ßn ph√¢n bi·ªát r√µ.
    2. H√£y ch·ªçn c√°c ƒëo·∫°n vƒÉn li√™n quan ƒë·∫øn c√¢u h·ªèi, ch√∫ √Ω ƒë·∫øn c√°c t·ª´ kho√° trong c√¢u h·ªèi v√† ti√™u ƒë·ªÅ ph·∫ßn/ch∆∞∆°ng trong t√†i li·ªáu. Ch·ªçn to√†n b·ªô c√°c ƒëo·∫°n vƒÉn c√≥ ch·ª©a t·ª´ kh√≥a ch√≠nh li√™n quan ƒë·∫øn c√¢u h·ªèi.
    3. Ph·∫£i r√† so√°t to√†n b·ªô n·ªôi dung ƒë·ªÉ t√¨m t·∫•t c·∫£ ti√™u ƒë·ªÅ ph·∫ßn n√†o c√≥ ch·ª©a th√¥ng tin li√™n quan ƒë·∫øn t·ª´ kh√≥a trong c√¢u h·ªèi. Kh√¥ng ƒë∆∞·ª£c ch·ªâ d·ª´ng l·∫°i ·ªü ph·∫ßn ƒë·∫ßu ti√™n t√¨m th·∫•y. N·∫øu nhi·ªÅu ph·∫ßn c√≥ li√™n quan, ph·∫£i ch·ªçn t·∫•t c·∫£.

    üî¥ 4. **B·∫Øt bu·ªôc gi·ªØ nguy√™n to√†n b·ªô c√°c n·ªôi dung c√≥ c·∫•u tr√∫c ph√¢n m·ª•c nh∆∞ `a)`, `b)`, `1.`, `2.`, ho·∫∑c m√£ s·ªë nh∆∞ `A.2.29.1`, `6.11.4`... N·∫øu m·ªôt m·ª•c nh∆∞ `A.2.29` ƒë∆∞·ª£c ch·ªçn, ph·∫£i bao g·ªìm to√†n b·ªô c√°c ti·ªÉu m·ª•c `A.2.29.x` b√™n d∆∞·ªõi. Kh√¥ng ƒë∆∞·ª£c b·ªè s√≥t ho·∫∑c r√∫t g·ªçn.**

    üî¥ 5. **Kh√¥ng ƒë∆∞·ª£c r√∫t g·ªçn, t√≥m t·∫Øt, gom nh√≥m ho·∫∑c di·ªÖn gi·∫£i l·∫°i n·ªôi dung theo ki·ªÉu chung chung. Ph·∫£i tr√≠ch nguy√™n vƒÉn ƒëo·∫°n vƒÉn ph√π h·ª£p.**

    6. **Tr·∫£ l·ªùi d∆∞·ªõi ƒë·ªãnh d·∫°ng JSON**, bao g·ªìm c√°c tr∆∞·ªùng sau:
    - `"summary_answer"`: 
        - Tr√≠ch nguy√™n vƒÉn c√°c ƒëo·∫°n c√≥ li√™n quan, gi·ªØ ƒë√∫ng c·∫•u tr√∫c ph√¢n m·ª•c ban ƒë·∫ßu.
        - N·∫øu ƒëo·∫°n vƒÉn ƒë∆∞·ª£c ch·ªçn ch·ª©a c√°c m·ª•c d·∫°ng `a), b), c)` ho·∫∑c `1., 2., 3.` ho·∫∑c c√°c d√≤ng c√≥ m√£ s·ªë nh∆∞ `A.2.29`, `A.2.29.1`, **ph·∫£i l·∫•y ƒë·∫ßy ƒë·ªß t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi m·ª•c ƒë√≥ v√† c√°c m·ª•c con b√™n d∆∞·ªõi**.

        - V·ªõi m·ªói vƒÉn b·∫£n v√† ti√™u ƒë·ªÅ trong ph·∫ßn `"results"`, tr√¨nh b√†y c√¢u tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng:
        ```
        **VƒÉn b·∫£n:** Document 1
        Ti√™u ƒë·ªÅ: Title 1
        - N·ªôi dung li√™n quan ƒë·∫ßy ƒë·ªß theo ƒë·ªãnh d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng, s·ªë m·ª•c ho·∫∑c m√£ m·ª•c...

        **VƒÉn b·∫£n:** Document 2
        Ti√™u ƒë·ªÅ: Title 2
        - ...
        ```
    - `"results"`: Danh s√°ch c√°c c·∫∑p `[document, title]`, trong ƒë√≥:
        - `document`: T√™n vƒÉn b·∫£n, l·∫•y t·ª´ ti√™u ƒë·ªÅ `# VƒÉn b·∫£n`.
        - `title`: Ti√™u ƒë·ªÅ ph·∫ßn ph√π h·ª£p nh·∫•t, l·∫•y t·ª´ `## Ti√™u ƒë·ªÅ`, kh√¥ng ƒë∆∞·ª£c ch·ªçn b√™n trong ƒëo·∫°n vƒÉn.
    - `"needAgent"`: `true` n·∫øu c·∫ßn chuy·ªÉn cho chuy√™n vi√™n x·ª≠ l√Ω.
    - `"needSearch"`: `true` n·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong n·ªôi dung tham kh·∫£o v√† c·∫ßn tra c·ª©u th√™m.

    7. **Kh√¥ng ƒë∆∞·ª£c suy lu·∫≠n ho·∫∑c th√™m th√¥ng tin ngo√†i t√†i li·ªáu ƒë√£ cho.**
    8. N·∫øu kh√°ch h√†ng y√™u c·∫ßu g·∫∑p chuy√™n vi√™n ho·∫∑c c√¢u h·ªèi v∆∞·ª£t qu√° kh·∫£ nƒÉng x·ª≠ l√Ω, tr·∫£ l·ªùi:
    > "T√¥i s·∫Ω g·ª≠i y√™u c·∫ßu c·ªßa b·∫°n ƒë·∫øn chuy√™n vi√™n chƒÉm s√≥c kh√°ch h√†ng. H·ªç s·∫Ω li√™n h·ªá v·ªõi b·∫°n trong th·ªùi gian s·ªõm nh·∫•t."
    v√† ƒë·∫∑t `"needAgent": true`.
    9. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong n·ªôi dung tham kh·∫£o, b·∫Øt bu·ªôc ƒë·∫∑t `"needSearch": true` v√† `"results"` ph·∫£i l√† danh s√°ch r·ªóng.

    üî¥ 10. **Quan tr·ªçng**: N·∫øu m·ªôt ph·∫ßn nh∆∞ `6.11` ho·∫∑c `A.2.29` ƒë∆∞·ª£c ch·ªçn th√¨ ph·∫£i bao g·ªìm ƒë·∫ßy ƒë·ªß t·∫•t c·∫£ c√°c ti·ªÉu m·ª•c t·ª´ `6.11.1` ƒë·∫øn `6.11.x`, ho·∫∑c `A.2.29.1` ƒë·∫øn `A.2.29.x`. Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c c·∫Øt b·ªõt.

    **ƒê·ªãnh d·∫°ng JSON tr·∫£ l·ªùi m·∫´u:**
    ```json
    {{
        "summary_answer": "**VƒÉn b·∫£n:** Document 1\\nTi√™u ƒë·ªÅ 1: Title 1\\n- M·ª•c a) ...\\n- M·ª•c b) ...\\n- A.2.29.1 ...\\n- A.2.29.2 ...",
        "results": [
            ["Document 1", "Title 1"]
        ],
        "needAgent": false,
        "needSearch": false
    }}

    N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p, b·∫Øt bu·ªôc ph·∫£i tr·∫£ v·ªÅ ƒë√∫ng m·∫´u:
    {{
        "summary_answer": "",
        "results": [],
        "needAgent": false,
        "needSearch": true
    }}

    C√¢u h·ªèi:
    {query}

    N·ªôi dung tham kh·∫£o:
    {context_block}
    """
        return prompt

    def make_rag_prompt_eng(self, query: str, context_block: str) -> str:
        prompt = f"""
            You are a professional virtual assistant, tasked with answering customer questions based strictly on the provided technical documents or legal texts below.

            Question:
            {query}

            Reference content:
            {context_block}

            Answer requirements:
            1. Carefully analyze the key terms in the question (including object names, actions, technical specifications, specific regulations, etc.) and **directly compare these with the section/chapter titles in the reference content** to select the most relevant documents and titles. Avoid general or unrelated titles.
            2. If there are multiple relevant titles or sections, **list all of them without omitting any important sections.**
            3. **For any section selected, you must include the entire content of that section, including all subpoints and numbered/lettered items (such as "A.2.29", "A.2.29.1", "6.11.4", "a)", "b)", etc.). Do not omit, summarize, paraphrase, or remove any items. Extract content verbatim.**
            4. Respond in JSON format including:
            - "summary_answer": The **verbatim extracted content** from all selected sections, maintaining the original structure, numbering, and formatting. For each document and section, present the answer as:
                ```
                **Document:** Document 1
                Section: Title 1
                - Full relevant content...

                **Document:** Document 2
                Section: Title 2
                - Full relevant content...
                ```
            - "results": A list of [document, title] pairs, where each pair is the document name (from "# Document") and the section title (from "## Title").
            - "needAgent": true if the question should be escalated to a human agent.
            - "needSearch": true if no relevant information is found and further research is needed.
            5. **Absolutely no inference, summarization, or addition of information not present in the reference text. Only base your answer on the provided content.**
            6. If the customer requests to meet an agent or similar, respond: "I will forward your request to the customer service agent. They will contact you as soon as possible." and set "needAgent": true.
            7. If no relevant information is found in the reference content, set "needSearch": true and "results" as an empty list. Do not answer in any other format.
            8. **Do not remove any subsections or subpoints if a section is selected. All subpoints (e.g., "A.2.29.1", "a)", "b)", etc.) must be included in full, with their original numbering and formatting.**

            Please format your answer in JSON like this example:

            {{
            "results": [
                ["Document 1", "Title 1"],
                ["Document 2", "Title 2"]
            ],
            "needAgent": false,
            "needSearch": false
            }}

            Remember: If no relevant information is found, you must return:

            {{
            "results": [],
            "needAgent": false,
            "needSearch": true
            }}
        """
        return prompt

    def get_full_section_content(self, doc_name, section_title, json_folder: str = "new-vn-data-json"):
        norm_title = self.normalize_title(section_title)
        doc_file = doc_name + ".json"
        doc_path = os.path.join(json_folder, doc_file)

        try:
            with open(doc_path, "r", encoding="utf-8") as f:
                doc_data = json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Warning: JSON file not found, skipping: {doc_path}")
            return f"Kh√¥ng t√¨m th·∫•y file {doc_path}"

        matching_section = self.find_matching_section_or_subsection(
            doc_data, norm_title)

        if matching_section:
            return "\n".join(self.collect_all_titles_and_texts(matching_section))

        return "[0] Kh√¥ng t√¨m th·∫•y ph·∫ßn n·ªôi dung n√†o ph√π h·ª£p v·ªõi ti√™u ƒë·ªÅ v√† t√†i li·ªáu ƒë√£ cho."

    def normalize_title(self, text: str) -> str:
        text = unicodedata.normalize('NFD', text.strip())
        text = ''.join(c for c in text if unicodedata.category(c)
                       != 'Mn')  # remove accents
        text = ''.join(c for c in text if not c.isspace())  # remove whitespace
        return text.lower()

    def find_matching_section_or_subsection(self, doc_data, norm_title):
        for section in doc_data.get("sections", []):
            result = self._find_in_node(section, norm_title)
            if result:
                return result
        return None

    def _find_in_node(self, node, norm_title):
        section_title = node.get("section") or node.get("subsection")
        if section_title:
            norm_section = self.normalize_title(section_title)
            similarity = difflib.SequenceMatcher(
                None, norm_section, norm_title).ratio()
            if similarity >= 0.9:
                return node

        for child in node.get("content", []):
            result = self._find_in_node(child, norm_title)
            if result:
                return result

        return None

    def collect_all_titles_and_texts(self, node):
        texts = []
        title = node.get("section") or node.get("subsection")
        if title:
            texts.append(title)
        if node.get("type") == "text":
            texts.append(node.get("data", ""))
        for child in node.get("content", []):
            texts.extend(self.collect_all_titles_and_texts(child))
        return texts

    def clean_markdown(self, text: str) -> str:
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    def filter_answer(
        self,
        full_sections,
        answer: str,
        json_folder: str = "new-vn-data-json",
        language: str = "vi"
    ) -> Tuple[str, str]:
        print("\nRaw answer:", answer)
        try:
            if isinstance(answer, str):
                answer = answer.strip().removeprefix("```json").removesuffix("```").strip()
                data = json.loads(answer)
            elif isinstance(answer, dict):
                data = answer
        except json.JSONDecodeError as e:
            print("JSON Decode Error:", str(e))
            return "C·∫ßn t√¨m ki·∫øm th√™m th√¥ng tin tr√™n m·∫°ng. Vui l√≤ng th·ª≠ l·∫°i sau.", ""

        results = data.get("results", [])
        need_agent = data.get("needAgent", False)
        need_search = data.get("needSearch", False)
        summary_answer = data.get("summary_answer", "")

        if need_search:
            return "C·∫ßn t√¨m ki·∫øm th√™m th√¥ng tin tr√™n m·∫°ng. Vui l√≤ng th·ª≠ l·∫°i sau.", ""
        if not results or summary_answer == "":
            return "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ho·∫∑c ti√™u ƒë·ªÅ li√™n quan trong c√¢u tr·∫£ l·ªùi.", ""

        doc_sections = {}
        for doc_title_pair in results:
            if not (isinstance(doc_title_pair, (list, tuple)) and len(doc_title_pair) == 2):
                continue
            document, guessed_title = doc_title_pair
            matched_section_title = None
            matched_body = None
            for section_text in full_sections:
                match = re.search(
                    r"# VƒÉn b·∫£n: (.*?)\n## Ti√™u ƒë·ªÅ: (.*?)\n\n ƒêo·∫°n vƒÉn: (.*)", section_text, re.DOTALL)
                if not match:
                    continue
                doc_name, real_title, body = match.groups()
                if doc_name.lower().strip() != document.lower().strip():
                    continue
                if guessed_title.strip() in body:
                    matched_section_title = real_title
                    matched_body = body
                    break
            if matched_section_title:
                norm_title = self.normalize_title(matched_section_title)
            doc_file = document.lower() + ".json"
            doc_path = os.path.join(json_folder, doc_file)
            try:
                with open(doc_path, "r", encoding="utf-8") as f:
                    doc_data = json.load(f)
            except FileNotFoundError:
                continue
            matching_section = self.find_matching_section_or_subsection(
                doc_data, norm_title)
            if matching_section:
                doc_sections.setdefault((document, doc_data.get(
                    "filename", document)), []).append(matching_section)

        if not doc_sections:
            return "[1] Kh√¥ng t√¨m th·∫•y ph·∫ßn n·ªôi dung n√†o ph√π h·ª£p v·ªõi ti√™u ƒë·ªÅ v√† t√†i li·ªáu ƒë√£ cho.", ""

        all_blocks = []
        for (document, display_name), sections in doc_sections.items():
            if language == "vi":
                header = "**VƒÉn b·∫£n:** " + display_name.upper()
            else:
                header = "**Document:** " + display_name.upper()
            contents = []
            for section in sections:
                section_text = "\n".join(
                    self.collect_all_titles_and_texts(section))
                contents.append(section_text)
            block = header + "\n\n" + "\n\n".join(contents)
            all_blocks.append(block)

        full_text = "\n\n---\n\n".join(all_blocks)
        if need_agent:
            full_text += "\n\n[Th√¥ng b√°o: Y√™u c·∫ßu ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn chuy√™n vi√™n t∆∞ v·∫•n.]"
        summary_answer = self.clean_markdown(summary_answer)
        full_text = self.clean_markdown(full_text)
        return summary_answer, full_text

    def get_relevant_sections_for_query(self, query, text_db, n_results=10, json_folder="new-vn-data-json"):
        """
        T·ªëi ∆∞u ch·ªçn context: l·∫•y c√°c section li√™n quan ƒë·∫øn ch·ªãu l·ª≠a ·ªëng gi√≥, ch·ªãu l·ª≠a EI, k·ªÉ c·∫£ khi LLM kh√¥ng match ƒë√∫ng ti√™u ƒë·ªÅ.
        - K·∫øt h·ª£p semantic (vector db) v√† keyword rule.
        - ∆Øu ti√™n l·∫•y c·∫£ nh·ªØng ƒëo·∫°n ch·ª©a c√°c t·ª´ kh√≥a k·ªπ thu·∫≠t "gi·ªõi h·∫°n ch·ªãu l·ª≠a", "EI", "·ªëng gi√≥", "·ªëng d·∫´n kh√≠", "k√™nh d·∫´n", ...
        """
        # ƒê·ªãnh nghƒ©a t·ª´ kh√≥a k·ªπ thu·∫≠t li√™n quan ch·ªãu l·ª≠a
        fire_keywords = [
            "gi·ªõi h·∫°n ch·ªãu l·ª≠a", "EI", "ch·ªãu l·ª≠a", "·ªëng gi√≥", "k√™nh d·∫´n", "·ªëng d·∫´n kh√≠",
            "van ngƒÉn kh√≥i", "van ngƒÉn ch√°y", "ph·∫£i ƒë·∫£m b·∫£o", "ƒë√°p ·ª©ng", "REI", "EI"
        ]
        # Lowercase h√≥a truy v·∫•n ƒë·ªÉ so kh·ªõp
        query_lc = query.lower()
        # L·∫•y n_results ƒë·∫ßu t·ª´ vector db
        text_res = text_db.query(query_texts=[query], n_results=n_results, include=[
                                 "documents", "metadatas", "distances"])
        text_doc = text_res['documents'][0]
        text_metadata = text_res['metadatas'][0]
        picked_sections = []

        for doc_text, meta in zip(text_doc, text_metadata):
            doc_name = meta.get("doc_name", "Unknown file")
            section_title = meta.get("section", "No Section Title")
            # T·∫£i to√†n b·ªô n·ªôi dung section (full text, c·∫£ ti√™u ƒë·ªÅ v√† c√°c ti·ªÉu m·ª•c)
            section_full_text = self.get_full_section_content(
                doc_name, section_title, json_folder=json_folder)
            section_text_lc = section_full_text.lower()
            section_title_lc = section_title.lower()
            # N·∫øu section ho·∫∑c n·ªôi dung c√≥ ch·ª©a t·ª´ kh√≥a k·ªπ thu·∫≠t, lu√¥n th√™m v√†o context
            if any(kw in section_title_lc or kw.lower() in section_text_lc for kw in fire_keywords):
                section_str = f"# VƒÉn b·∫£n: {doc_name.upper()}\n## Ti√™u ƒë·ªÅ: {section_title}\n\n ƒêo·∫°n vƒÉn: {section_full_text}"
                picked_sections.append(section_str)
            # N·∫øu similarity ti√™u ƒë·ªÅ v·ªõi truy v·∫•n l·ªõn h∆°n 0.7 c≈©ng l·∫•y
            elif difflib.SequenceMatcher(None, section_title_lc, query_lc).ratio() > 0.7:
                section_str = f"# VƒÉn b·∫£n: {doc_name.upper()}\n## Ti√™u ƒë·ªÅ: {section_title}\n\n ƒêo·∫°n vƒÉn: {section_full_text}"
                picked_sections.append(section_str)

        # Lo·∫°i b·ªè duplicate (b·∫±ng n·ªôi dung ƒëo·∫°n)
        uniq_sections = []
        seen_hashes = set()
        for sec in picked_sections:
            sec_hash = hash(sec)
            if sec_hash not in seen_hashes:
                uniq_sections.append(sec)
                seen_hashes.add(sec_hash)
        return uniq_sections

    def combined_answer(self, text_db, bm25_plus, full_documents_bm25, query, text_n_results=7, json_folder="new-vn-data-json", language="vi"):
        t0 = time.time()
        # 1. Normalize ONCE
        if language == "vi":
            normalized_query = self.normalize_viet(query)
        else:
            normalized_query = self.normalize_eng(query)
        t1 = time.time()
        print(f"‚è±Ô∏è Time to normalize query: {t1 - t0:.2f}s")

        # 2. Retrieve ONCE
        text_res = text_db.query(query_texts=[normalized_query], n_results=text_n_results, include=[
                                 "documents", "metadatas", "distances"])
        t2 = time.time()
        print(f"‚è±Ô∏è Vector DB query time: {t2 - t1:.2f}s")

        # 3. Assemble context blocks
        # full_sections = []
        # text_doc = text_res['documents'][0]
        # text_metadata = text_res['metadatas'][0]
        # for doc_text, meta in zip(text_doc, text_metadata):
        #     doc_name = meta.get("doc_name", "Unknown file")
        #     section_title = meta.get("section", "No Section Title")
        #     section_text = self.get_full_section_content(
        #         doc_name, section_title)
        #     if language == "vi":
        #         section_text = f"# VƒÉn b·∫£n: {doc_name}\n## Ti√™u ƒë·ªÅ: {section_title}\n\n ƒêo·∫°n vƒÉn: {section_text}"
        #     else:
        #         section_text = f"# Document: {doc_name}\n## Title: {section_title}\n\n Text: {section_text}"
        #     if section_text:
        #         full_sections.append(section_text)
        # combined_context = "\n----\n".join(full_sections)

        # 3. Assemble context blocks
        full_sections = self.get_relevant_sections_for_query(
            normalized_query, text_db, n_results=text_n_results, json_folder=json_folder
        )
        combined_context = "\n----\n".join(full_sections)

        # 4. Prompt LLM ONCE
        if language == "vi":
            prompt = self.make_rag_prompt_viet(query, combined_context)
        else:
            prompt = self.make_rag_prompt_eng(query, combined_context)

        answer = self.generate_model_answer(prompt)
        t3 = time.time()
        print(f"‚è±Ô∏è LLM answer time: {t3 - t2:.2f}s")

        # 5. Filter/format answer (if required)
        summary_answer, references = self.filter_answer(
            full_sections, answer, json_folder=json_folder, language=language)
        t4 = time.time()
        print(f"‚è±Ô∏è Total time: {t4 - t0:.2f}s")

        return summary_answer, references
