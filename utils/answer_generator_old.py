import re
import difflib
import time
import unicodedata
import os
import json
import numpy as np
from pyvi import ViTokenizer

from collections import defaultdict
from typing import Dict, Tuple

from config import Config
from utils.search_query import QueryProcessor
from typing import List
from utils.embedding import BM25EmbeddingFunction


class AnswerGenerator:
    def __init__(self):
        self.config = Config()
        self.model = self.config.get_model_used()
        self.query_processor = QueryProcessor()
        self.FALLBACK_TRIGGERS = [
            "tÃ´i chÆ°a tÃ¬m tháº¥y thÃ´ng tin",
            "tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin",
            "ráº¥t tiáº¿c, hiá»‡n táº¡i tÃ´i chÆ°a",
            "báº¡n vui lÃ²ng liÃªn há»‡ chuyÃªn viÃªn",
            "tÃ´i xin lá»—i",
            "vui lÃ²ng liÃªn há»‡ chuyÃªn viÃªn"
        ]

    # def generate_openai_answer(self, prompt: str) -> str:
    #     response = self.model.chat.completions.create(
    #         model="gpt-4o-mini",
    #         messages=[{"role": "user", "content": prompt}],
    #         max_tokens=1000
    #     )
    #     return response.choices[0].message.content.strip()

    def generate_openai_answer(self, prompt: str) -> str:
        start = time.time()
        response = self.model.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2048
        )
        print(f"â±ï¸ OpenAI generation time: {time.time() - start:.2f}s")
        return response.choices[0].message.content.strip()

    def generate_gemini_answer(self, prompt: str):
        response = self.model.generate_content(prompt)
        return response.text

    def generate_model_answer(self, prompt: str) -> str:
        if self.config.MODEL_USED == "gemini":
            return self.generate_gemini_answer(prompt)
        elif self.config.MODEL_USED == "openai":
            return self.generate_openai_answer(prompt)
        else:
            raise ValueError(f"Unsupported model: {self.config.MODEL_USED}")

    def normalize_viet(self, query: str) -> str:
        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ chuyÃªn xá»­ lÃ½ ngÃ´n ngá»¯ chuyÃªn ngÃ nh phÃ²ng chÃ¡y chá»¯a chÃ¡y (PCCC).
        
        HÃ£y viáº¿t láº¡i truy váº¥n dÆ°á»›i Ä‘Ã¢y thÃ nh má»™t cÃ¢u há»i Ä‘áº§y Ä‘á»§, rÃµ rÃ ng, sá»­ dá»¥ng ngÃ´n ngá»¯ ká»¹ thuáº­t chuáº©n nhÆ° trong cÃ¡c tÃ i liá»‡u QCVN hoáº·c TCVN.
        
        Má»¥c tiÃªu:
        - Giá»¯ nguyÃªn tá»« khÃ³a chÃ­nh vÃ  ná»™i dung gá»‘c cá»§a truy váº¥n.
        - Bá»• sung Ä‘áº§y Ä‘á»§ bá»‘i cáº£nh, tÃ¬nh huá»‘ng hoáº·c Ä‘iá»u kiá»‡n náº¿u cÃ³ thá»ƒ, Ä‘á»ƒ giÃºp truy váº¥n dá»… Ä‘Æ°á»£c tÃ¬m tháº¥y trong tÃ i liá»‡u ká»¹ thuáº­t.
        - TrÃ¡nh viáº¿t táº¯t, tá»« lÃ³ng hoáº·c ngÃ´n ngá»¯ Ä‘á»‹a phÆ°Æ¡ng.
        
        Chá»‰ tráº£ vá» truy váº¥n Ä‘Ã£ Ä‘Æ°á»£c viáº¿t láº¡i, khÃ´ng cáº§n giáº£i thÃ­ch.

        VÃ­ dá»¥:
        Truy váº¥n gá»‘c: "HÃ nh lang bÃªn lÃ  gÃ¬"  
        Truy váº¥n chuáº©n hÃ³a: "Theo quy chuáº©n, tiÃªu chuáº©n hiá»‡n hÃ nh vá» phÃ²ng chÃ¡y chá»¯a chÃ¡y, hÃ nh lang bÃªn cá»§a nhÃ  hoáº·c cÃ´ng trÃ¬nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° tháº¿ nÃ o vÃ  cÃ³ yÃªu cáº§u gÃ¬ vá» kÃ­ch thÆ°á»›c, váº­t liá»‡u xÃ¢y dá»±ng, vÃ  kháº£ nÄƒng chá»‹u lá»­a?"

        Truy váº¥n gá»‘c: "{query}"

        Truy váº¥n Ä‘Ã£ má»Ÿ rá»™ng vÃ  chuáº©n hÃ³a:
        """

        response = self.generate_model_answer(prompt)
        return response

    def normalize_eng(self, query: str) -> str:
        prompt = f"""
        You are a professional assistant specializing in fire prevention and fighting (PCCC) technical language.
        Rewrite the following query clearly, avoiding abbreviations or local dialects.
        The goal is to make this query easily understandable and searchable in standard technical documents like QCVN, TCVN.
        Only return the standardized question, no additional explanations.

        Original query: "{query}"

        Standardized query (using clear technical language):
        """

        response = self.generate_model_answer(prompt)
        return response

    def make_rag_prompt_viet(self, query: str, context_block: str) -> str:
        prompt = f"""
    Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o chuyÃªn nghiá»‡p, cÃ³ nhiá»‡m vá»¥ tráº£ lá»i cÃ¢u há»i cá»§a khÃ¡ch hÃ ng dá»±a trÃªn ná»™i dung tÃ i liá»‡u ká»¹ thuáº­t hoáº·c vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.

    **HÆ°á»›ng dáº«n tráº£ lá»i:**
    1. **PhÃ¢n tÃ­ch ká»¹ cÃ¢u há»i vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c tá»« khÃ³a quan trá»ng.** So sÃ¡nh trá»±c tiáº¿p cÃ¡c tá»« khÃ³a nÃ y vá»›i tiÃªu Ä‘á» pháº§n/chÆ°Æ¡ng trong tÃ i liá»‡u Ä‘á»ƒ tÃ¬m pháº§n phÃ¹ há»£p nháº¥t. VÃ­ dá»¥, **hÃºt khÃ³i** khÃ¡c vá»›i **á»‘ng thÃ´ng giÃ³** â€” cáº§n phÃ¢n biá»‡t rÃµ.
    2. HÃ£y chá»n cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan Ä‘áº¿n cÃ¢u há»i, chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« khoÃ¡ trong cÃ¢u há»i vÃ  tiÃªu Ä‘á» pháº§n/chÆ°Æ¡ng trong tÃ i liá»‡u. Chá»n toÃ n bá»™ cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ chá»©a tá»« khÃ³a chÃ­nh liÃªn quan Ä‘áº¿n cÃ¢u há»i.
    3. Pháº£i rÃ  soÃ¡t toÃ n bá»™ ná»™i dung Ä‘á»ƒ tÃ¬m táº¥t cáº£ tiÃªu Ä‘á» pháº§n nÃ o cÃ³ chá»©a thÃ´ng tin liÃªn quan Ä‘áº¿n tá»« khÃ³a trong cÃ¢u há»i. KhÃ´ng Ä‘Æ°á»£c chá»‰ dá»«ng láº¡i á»Ÿ pháº§n Ä‘áº§u tiÃªn tÃ¬m tháº¥y. Náº¿u nhiá»u pháº§n cÃ³ liÃªn quan, pháº£i chá»n táº¥t cáº£.
    4. Náº¿u cÃ¢u tráº£ lá»i cÃ³ table, hÃ£y format láº¡i Ä‘Ãºng Ä‘á»‹nh dáº¡ng Markdown cá»§a báº£ng, bao gá»“m cÃ¡c tiÃªu Ä‘á» cá»™t vÃ  hÃ ng. Äáº£m báº£o giá»¯ nguyÃªn cáº¥u trÃºc báº£ng.

    ğŸ”´ 4. **Báº¯t buá»™c giá»¯ nguyÃªn toÃ n bá»™ cÃ¡c ná»™i dung cÃ³ cáº¥u trÃºc phÃ¢n má»¥c nhÆ° `a)`, `b)`, `1.`, `2.`, hoáº·c mÃ£ sá»‘ nhÆ° `A.2.29.1`, `6.11.4`... Náº¿u má»™t má»¥c nhÆ° `A.2.29` Ä‘Æ°á»£c chá»n, pháº£i bao gá»“m toÃ n bá»™ cÃ¡c tiá»ƒu má»¥c `A.2.29.x` bÃªn dÆ°á»›i. KhÃ´ng Ä‘Æ°á»£c bá» sÃ³t hoáº·c rÃºt gá»n.**

    ğŸ”´ 5. **KhÃ´ng Ä‘Æ°á»£c rÃºt gá»n, tÃ³m táº¯t, gom nhÃ³m hoáº·c diá»…n giáº£i láº¡i ná»™i dung theo kiá»ƒu chung chung. Pháº£i trÃ­ch nguyÃªn vÄƒn Ä‘oáº¡n vÄƒn phÃ¹ há»£p.**

    ğŸ”´ 6. Náº¿u Ä‘oáº¡n vÄƒn cÃ³ chá»©a hÃ¬nh áº£nh Markdown (vÃ­ dá»¥: ![caption](path/to/image.png)), báº¯t buá»™c pháº£i giá»¯ nguyÃªn Ä‘á»‹nh dáº¡ng hÃ¬nh áº£nh nÃ y trong 'summary_answer'. KhÃ´ng Ä‘Æ°á»£c xÃ³a hoáº·c thay Ä‘á»•i Ä‘Æ°á»ng dáº«n hÃ¬nh áº£nh.
    
    ğŸ”´ 7. Náº¿u cÃ¢u há»i cÃ³ tÃ­nh cháº¥t tá»•ng quÃ¡t nhÆ° "phÃ¢n loáº¡i nhÃ³m nhÃ ", pháº£i má»Ÿ rá»™ng tÃ¬m kiáº¿m Ä‘áº¿n táº¥t cáº£ cÃ¡c tiÃªu Ä‘á» vÃ  ná»™i dung liÃªn quan Ä‘áº¿n khÃ¡i niá»‡m chÃ­nh (vÃ­ dá»¥: cÃ´ng nÄƒng, chiá»u cao, káº¿t cáº¥u, váº­t liá»‡u...). KhÃ´ng chá»‰ chá»n pháº§n chá»©a Ä‘Ãºng cá»¥m tá»« cÃ¢u há»i, mÃ  pháº£i bao quÃ¡t toÃ n bá»™ cÃ¡c tiÃªu chÃ­ phÃ¢n loáº¡i liÃªn quan. Pháº£i giá»¯ url áº£nh markdown.

    8. **Tráº£ lá»i dÆ°á»›i Ä‘á»‹nh dáº¡ng JSON**, bao gá»“m cÃ¡c trÆ°á»ng sau:
    - `"summary_answer"`: 
        - TrÃ­ch nguyÃªn vÄƒn cÃ¡c Ä‘oáº¡n cÃ³ liÃªn quan, giá»¯ Ä‘Ãºng cáº¥u trÃºc phÃ¢n má»¥c ban Ä‘áº§u.
        - Náº¿u Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c chá»n chá»©a cÃ¡c má»¥c dáº¡ng `a), b), c)` hoáº·c `1., 2., 3.` hoáº·c cÃ¡c dÃ²ng cÃ³ mÃ£ sá»‘ nhÆ° `A.2.29`, `A.2.29.1`, **pháº£i láº¥y Ä‘áº§y Ä‘á»§ tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i má»¥c Ä‘Ã³ vÃ  cÃ¡c má»¥c con bÃªn dÆ°á»›i**.

        - Vá»›i má»—i vÄƒn báº£n vÃ  tiÃªu Ä‘á» trong pháº§n `"results"`, trÃ¬nh bÃ y cÃ¢u tráº£ lá»i theo Ä‘á»‹nh dáº¡ng:
        ```
        **VÄƒn báº£n:** Document 1
        TiÃªu Ä‘á»: Title 1
        - Ná»™i dung liÃªn quan Ä‘áº§y Ä‘á»§ theo Ä‘á»‹nh dáº¡ng gáº¡ch Ä‘áº§u dÃ²ng, sá»‘ má»¥c hoáº·c mÃ£ má»¥c...

        **VÄƒn báº£n:** Document 2
        TiÃªu Ä‘á»: Title 2
        - ...
        ```
    - `"results"`: Danh sÃ¡ch cÃ¡c cáº·p `[document, title]`, trong Ä‘Ã³:
        - `document`: TÃªn vÄƒn báº£n, láº¥y tá»« tiÃªu Ä‘á» `# VÄƒn báº£n`.
        - `title`: TiÃªu Ä‘á» pháº§n phÃ¹ há»£p nháº¥t, láº¥y tá»« `## TiÃªu Ä‘á»`, khÃ´ng Ä‘Æ°á»£c chá»n bÃªn trong Ä‘oáº¡n vÄƒn.
    - `"needAgent"`: `true` náº¿u cáº§n chuyá»ƒn cho chuyÃªn viÃªn xá»­ lÃ½.
    - `"needSearch"`: `true` náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong ná»™i dung tham kháº£o vÃ  cáº§n tra cá»©u thÃªm.

    9. **KhÃ´ng Ä‘Æ°á»£c suy luáº­n hoáº·c thÃªm thÃ´ng tin ngoÃ i tÃ i liá»‡u Ä‘Ã£ cho.**
    10. Náº¿u khÃ¡ch hÃ ng yÃªu cáº§u gáº·p chuyÃªn viÃªn hoáº·c cÃ¢u há»i vÆ°á»£t quÃ¡ kháº£ nÄƒng xá»­ lÃ½, tráº£ lá»i:
    > "TÃ´i sáº½ gá»­i yÃªu cáº§u cá»§a báº¡n Ä‘áº¿n chuyÃªn viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng. Há» sáº½ liÃªn há»‡ vá»›i báº¡n trong thá»i gian sá»›m nháº¥t."
    vÃ  Ä‘áº·t `"needAgent": true`.
    11. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong ná»™i dung tham kháº£o, báº¯t buá»™c Ä‘áº·t `"needSearch": true` vÃ  `"results"` pháº£i lÃ  danh sÃ¡ch rá»—ng.

    ğŸ”´ 12. **Quan trá»ng**: Náº¿u má»™t pháº§n nhÆ° `6.11` hoáº·c `A.2.29` Ä‘Æ°á»£c chá»n thÃ¬ pháº£i bao gá»“m Ä‘áº§y Ä‘á»§ táº¥t cáº£ cÃ¡c tiá»ƒu má»¥c tá»« `6.11.1` Ä‘áº¿n `6.11.x`, hoáº·c `A.2.29.1` Ä‘áº¿n `A.2.29.x`. Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c cáº¯t bá»›t.
    
    **Äá»‹nh dáº¡ng JSON tráº£ lá»i máº«u:**
    ```json
    {{
        "summary_answer": "**VÄƒn báº£n:** Document 1\\nTiÃªu Ä‘á» 1: Title 1\\n- Má»¥c a) ...\\n- Má»¥c b) ...\\n- A.2.29.1 ...\\n- A.2.29.2 ...",
        "results": [
            ["Document 1", "Title 1"]
        ],
        "needAgent": false,
        "needSearch": false
    }}

    Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p, báº¯t buá»™c pháº£i tráº£ vá» Ä‘Ãºng máº«u:
    {{
        "summary_answer": "",
        "results": [],
        "needAgent": false,
        "needSearch": true
    }}

    CÃ¢u há»i:
    {query}

    Ná»™i dung tham kháº£o:
    {context_block}
    """
        return prompt

    def voted_original_normalized_answer(self, query, summary_answer, normalized_answer):
        prompt = f"""
    Báº¡n lÃ  má»™t chuyÃªn gia trong lÄ©nh vá»±c PhÃ²ng chÃ¡y chá»¯a chÃ¡y (PCCC), am hiá»ƒu sÃ¢u sáº¯c cÃ¡c tÃ i liá»‡u phÃ¡p lÃ½ vÃ  ká»¹ thuáº­t nhÆ° Quy chuáº©n (QC), TiÃªu chuáº©n (TCVN), Nghá»‹ Ä‘á»‹nh, ThÃ´ng tÆ°,...

    DÆ°á»›i Ä‘Ã¢y lÃ  má»™t cÃ¢u há»i gá»‘c cá»§a ngÆ°á»i dÃ¹ng vÃ  hai cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o ra tá»« hai phiÃªn báº£n khÃ¡c nhau cá»§a cÃ¢u há»i: báº£n gá»‘c vÃ  báº£n Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hoÃ¡ láº¡i Ä‘á»ƒ tÃ¬m kiáº¿m tá»‘t hÆ¡n.

    ## CÃ¢u há»i gá»‘c:
    {query}

    ## Tráº£ lá»i tá»« cÃ¢u há»i gá»‘c (dÆ°á»›i dáº¡ng JSON):
    ```json
    {summary_answer}

    ## Tráº£ lá»i tá»« cÃ¢u há»i Ä‘Ã£ chuáº©n hoÃ¡ (dÆ°á»›i dáº¡ng JSON):
    ```json
    {normalized_answer}

    ## Nhiá»‡m vá»¥ cá»§a báº¡n:

    1. Äá»c ká»¹ cÃ¢u há»i gá»‘c Ä‘á»ƒ xÃ¡c Ä‘á»‹nh ná»™i dung chÃ­nh cáº§n tráº£ lá»i.
    2. ÄÃ¡nh giÃ¡ ná»™i dung cá»§a cáº£ hai cÃ¢u tráº£ lá»i vÃ  **tá»•ng há»£p láº¡i cÃ¡c pháº§n phÃ¹ há»£p nháº¥t vá»›i cÃ¢u há»i gá»‘c**, Ä‘áº£m báº£o **khÃ´ng bá» sÃ³t hoáº·c tÃ³m táº¯t báº¥t ká»³ má»¥c, phÃ¢n má»¥c hoáº·c tiá»ƒu má»¥c nÃ o**. KhÃ´ng Ä‘Æ°á»£c bá» qua cÃ¡c Ä‘Ã¡nh sá»‘, kÃ½ hiá»‡u má»¥c nhÆ° "A.2.29", "A.2.29.1", "6.11.4", "a)", "b)",... Náº¿u má»™t má»¥c hoáº·c phÃ¢n má»¥c Ä‘Æ°á»£c chá»n, báº¡n pháº£i **giá»¯ nguyÃªn toÃ n bá»™ ná»™i dung vÃ  cÃ¡c má»¥c con bÃªn dÆ°á»›i**, khÃ´ng Ä‘Æ°á»£c rÃºt gá»n hoáº·c bá» qua.
    3. KhÃ´ng chá»‰ sao chÃ©p, cÅ©ng khÃ´ng Ä‘Æ°á»£c cáº¯t xÃ©n, mÃ  pháº£i **giá»¯ láº¡i toÃ n bá»™ cÃ¡c ná»™i dung liÃªn quan, báº£o toÃ n cáº¥u trÃºc gá»‘c, bao gá»“m má»i phÃ¢n má»¥c vÃ  Ä‘Ã¡nh sá»‘/formatting ban Ä‘áº§u**.
    4. Tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng JSON vá»›i cÃ¡c trÆ°á»ng sau:
        - "summary_answer": cÃ¢u tráº£ lá»i má»›i Ä‘Æ°á»£c tá»•ng há»£p láº¡i, **trÃ­ch nguyÃªn vÄƒn, giá»¯ Ä‘Ãºng Ä‘á»‹nh dáº¡ng markdown vÃ  má»i phÃ¢n má»¥c, tiá»ƒu má»¥c** nhÆ° ban Ä‘áº§u.
        - "results": danh sÃ¡ch cÃ¡c cáº·p [document_id, document_title] liÃªn quan trá»±c tiáº¿p Ä‘áº¿n pháº§n ná»™i dung Ä‘Æ°á»£c giá»¯ láº¡i.
        - "needAgent" vÃ  "needSearch": giá»¯ nguyÃªn hoáº·c cáº­p nháº­t logic náº¿u má»™t trong hai cÃ¢u tráº£ lá»i cÃ³ true.

    ## Äá»‹nh dáº¡ng JSON máº«u:
    ```json
    {{
        "summary_answer": "**VÄƒn báº£n:** Document X\\n- Äáº§y Ä‘á»§ ná»™i dung, giá»¯ nguyÃªn má»i phÃ¢n má»¥c, tiá»ƒu má»¥c...\\n- A.2.29 ...\\n- A.2.29.1 ...",
        "results": [
            ["Document X", "TiÃªu Ä‘á» X"],
            ["Document Y", "TiÃªu Ä‘á» Y"]
        ],
        "needAgent": false,
        "needSearch": false
    }}
    """
        return prompt

    def make_rag_prompt_eng(self, query: str, context_block: str) -> str:
        prompt = f"""
            You are a professional virtual assistant, tasked with answering customer questions based strictly on the provided technical documents or legal texts below.

            Question:
            {query}

            Reference content:
            {context_block}

            Answer requirements:
            1. Carefully analyze the key terms in the question (including object names, actions, technical specifications, specific regulations, etc.) and **directly compare these with the section/chapter titles in the reference content** to select the most relevant documents and titles. Avoid general or unrelated titles.
            2. If there are multiple relevant titles or sections, **list all of them without omitting any important sections.**
            3. **For any section selected, you must include the entire content of that section, including all subpoints and numbered/lettered items (such as "A.2.29", "A.2.29.1", "6.11.4", "a)", "b)", etc.). Do not omit, summarize, paraphrase, or remove any items. Extract content verbatim.**
            4. Respond in JSON format including:
            - "summary_answer": The **verbatim extracted content** from all selected sections, maintaining the original structure, numbering, and formatting. For each document and section, present the answer as:
                ```
                **Document:** Document 1
                Section: Title 1
                - Full relevant content...

                **Document:** Document 2
                Section: Title 2
                - Full relevant content...
                ```
            - "results": A list of [document, title] pairs, where each pair is the document name (from "# Document") and the section title (from "## Title").
            - "needAgent": true if the question should be escalated to a human agent.
            - "needSearch": true if no relevant information is found and further research is needed.
            5. **Absolutely no inference, summarization, or addition of information not present in the reference text. Only base your answer on the provided content.**
            6. If the customer requests to meet an agent or similar, respond: "I will forward your request to the customer service agent. They will contact you as soon as possible." and set "needAgent": true.
            7. If no relevant information is found in the reference content, set "needSearch": true and "results" as an empty list. Do not answer in any other format.
            8. **Do not remove any subsections or subpoints if a section is selected. All subpoints (e.g., "A.2.29.1", "a)", "b)", etc.) must be included in full, with their original numbering and formatting.**

            Please format your answer in JSON like this example:

            {{
            "results": [
                ["Document 1", "Title 1"],
                ["Document 2", "Title 2"]
            ],
            "needAgent": false,
            "needSearch": false
            }}

            Remember: If no relevant information is found, you must return:

            {{
            "results": [],
            "needAgent": false,
            "needSearch": true
            }}
        """
        return prompt

    def make_rag_prompt_with_history(query: str, context_block: List[str], history: List[str]) -> str:
        history_text = "\n".join(history[:-5]) if history else ""
        context_block = "\n---\n".join(context_block)

        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o chÄƒm sÃ³c khÃ¡ch hÃ ng.

        Lá»‹ch sá»­ trÃ² chuyá»‡n:
        {history_text}

        Ná»™i dung tham kháº£o:
        {context_block}

        CÃ¢u há»i:
        {query}

        HÃ£y tráº£ lá»i má»™t cÃ¡ch lá»‹ch sá»±, chu Ä‘Ã¡o vÃ  chÃ­nh xÃ¡c:
        - Náº¿u khÃ¡ch hÃ ng nÃ³i "tÃ´i muá»‘n gáº·p chuyÃªn viÃªn" hoáº·c tÆ°Æ¡ng tá»±, tráº£ lá»i: "TÃ´i sáº½ gá»­i yÃªu cáº§u cá»§a báº¡n Ä‘áº¿n chuyÃªn viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng. Há» sáº½ liÃªn há»‡ vá»›i báº¡n trong thá»i gian sá»›m nháº¥t."
        - Náº¿u cÃ³ thÃ´ng tin phÃ¹ há»£p, tráº£ lá»i Ä‘áº§y Ä‘á»§ vÃ  chÃ­nh xÃ¡c, khÃ´ng thÃªm suy diá»…n ngoÃ i ná»™i dung tham kháº£o.
        - Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, Ä‘á» nghá»‹ khÃ¡ch hÃ ng liÃªn há»‡ nhÃ¢n viÃªn tÆ° váº¥n.

        Vui lÃ²ng Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i dÆ°á»›i dáº¡ng JSON:
        {{
            "response": "<CÃ¢u tráº£ lá»i>",
            "needAgent": <true/false>,
        }}
    """
        return prompt

    def get_full_section_content(self, doc_name, section_title, json_folder: str = "new-vn-data-json"):
        # âœ… 1) Group matches by document
        doc_sections = {}  # dict: { document : [sections...] }

        doc_file = doc_name + ".json"
        doc_path = os.path.join(json_folder, doc_file)

        try:
            with open(doc_path, "r", encoding="utf-8") as f:
                doc_data = json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ Warning: JSON file not found, skipping: {doc_path}")
            return f"KhÃ´ng tÃ¬m tháº¥y file {doc_path}"

        norm_title = self.normalize_title(section_title)
        matching_section = self.find_matching_section_or_subsection(
            doc_data, norm_title)

        if matching_section:
            doc_sections.setdefault((doc_name, doc_data.get(
                "filename", doc_name)), []).append(matching_section)
        else:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á» phÃ¹ há»£p cho: '{section_title}' (chuáº©n hoÃ¡: '{norm_title}')")

        if not doc_sections:
            return "KhÃ´ng tÃ¬m tháº¥y pháº§n ná»™i dung nÃ o phÃ¹ há»£p vá»›i tiÃªu Ä‘á» vÃ  tÃ i liá»‡u Ä‘Ã£ cho."

        # âœ… 2) Build blocks, one per document, all its sections inside
        all_blocks = []
        for (document, display_name), sections in doc_sections.items():
            # Gather texts of all matched sections
            contents = []
            for section in sections:
                section_text = "\n".join(
                    self.collect_all_titles_and_texts(section))
                contents.append(section_text)

            block = "\n\n".join(contents)
            all_blocks.append(block)

        # âœ… 3) Join blocks with ---
        full_text = "\n\n---\n\n".join(all_blocks)

        return full_text

    def strip_accents(text):
        return ''.join(
            c for c in unicodedata.normalize('NFD', text)
            if unicodedata.category(c) != 'Mn'
        )

    def normalize_title(self, text: str) -> str:
        text = unicodedata.normalize('NFD', text.strip())
        text = ''.join(c for c in text if unicodedata.category(c)
                       != 'Mn')  # remove accents
        # remove all whitespace chars
        text = ''.join(c for c in text if not c.isspace())
        return text.lower()

    def find_matching_section_or_subsection(self, doc_data, norm_title):
        """
        Search through all top-level sections and their nested content 
        to find a section or subsection whose normalized title matches norm_title.
        """
        for section in doc_data.get("sections", []):
            result = self._find_in_node(section, norm_title)
            if result:
                return result
        return None

    def _find_in_node(self, node, norm_title):
        """
        Recursively check this node and its children for a similarity match >= 80%.
        """
        section_title = node.get("section") or node.get("subsection")
        if section_title:
            norm_section = self.normalize_title(section_title)
            similarity = difflib.SequenceMatcher(
                None, norm_section, norm_title).ratio()
            if similarity >= 0.9:
                return node

        for child in node.get("content", []):
            result = self._find_in_node(child, norm_title)
            if result:
                return result

        return None

    def filter_answer(
        self,
        full_sections,
        answer: str,
        json_folder: str = "new-vn-data-json",
        language: str = "vi"
    ) -> Tuple[str, str]:

        # answer = answer.strip().removeprefix("```json").removesuffix("```").strip()

        print("\nRaw answer:", answer)

        try:
            if isinstance(answer, str):
                answer = answer.strip().removeprefix("```json").removesuffix("```").strip()
                data = json.loads(answer)
            elif isinstance(answer, dict):
                data = answer
        except json.JSONDecodeError as e:
            print("JSON Decode Error:", str(e))
            return "Cáº§n tÃ¬m kiáº¿m thÃªm thÃ´ng tin trÃªn máº¡ng. Vui lÃ²ng thá»­ láº¡i sau."

        results = data.get("results", [])
        need_agent = data.get("needAgent", False)
        need_search = data.get("needSearch", False)
        summary_answer = data.get("summary_answer", "")

        print("\nSUMMARY ANSWER: ", summary_answer)

        if need_search:
            return "Cáº§n tÃ¬m kiáº¿m thÃªm thÃ´ng tin trÃªn máº¡ng. Vui lÃ²ng thá»­ láº¡i sau.", ""
        if not results or summary_answer == "":
            return "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u hoáº·c tiÃªu Ä‘á» liÃªn quan trong cÃ¢u tráº£ lá»i.", ""

        # âœ… 1) Group matches by document
        doc_sections = {}  # dict: { (document, filename) : [sections...] }

        for doc_title_pair in results:
            if not (isinstance(doc_title_pair, (list, tuple)) and len(doc_title_pair) == 2):
                continue

            document, guessed_title = doc_title_pair
            matched_section_title = None
            matched_body = None

            # âœ… DÃ² láº¡i Ä‘Ãºng tiÃªu Ä‘á» tá»« full_sections
            for section_text in full_sections:
                match = re.search(
                    r"# VÄƒn báº£n: (.*?)\n## TiÃªu Ä‘á»: (.*?)\n\nÄoáº¡n vÄƒn: (.*)", section_text, re.DOTALL)
                if not match:
                    continue
                doc_name, real_title, body = match.groups()

                print(f"Checking document: {doc_name}, guessed title: {guessed_title}, real title: {real_title}")

                # Match Ä‘Ãºng vÄƒn báº£n
                if doc_name.strip() != document.strip():
                    continue

                if guessed_title.strip() == real_title.strip():
                    matched_section_title = real_title
                    matched_body = body
                    break

                # âœ… Fallback: Náº¿u guessed_title náº±m trong ná»™i dung Ä‘oáº¡n vÄƒn
                if guessed_title.strip() in body:
                    matched_section_title = real_title
                    matched_body = body
                    break

            if matched_section_title:
                print(
                    f"Matched section title: {matched_section_title} for document: {document}")
                norm_title = self.normalize_title(matched_section_title)
            else:
                print(f"[âš ï¸] KhÃ´ng tÃ¬m Ä‘Æ°á»£c matched_section_title cho document: {document}, guessed_title: {guessed_title}")
                continue

            doc_file = document + ".json"
            doc_path = os.path.join(json_folder, doc_file)

            try:
                with open(doc_path, "r", encoding="utf-8") as f:
                    doc_data = json.load(f)
            except FileNotFoundError:
                continue

            matching_section = self.find_matching_section_or_subsection(
                doc_data, norm_title)

            if matching_section:
                doc_sections.setdefault((document, doc_data.get(
                    "filename", document)), []).append(matching_section)

        if not doc_sections:
            return "KhÃ´ng tÃ¬m tháº¥y pháº§n ná»™i dung nÃ o phÃ¹ há»£p vá»›i tiÃªu Ä‘á» vÃ  tÃ i liá»‡u Ä‘Ã£ cho.", ""

        # âœ… 2) Build blocks, one per document, all its sections inside
        all_blocks = []
        for (document, display_name), sections in doc_sections.items():
            if language == "vi":
                header = "**VÄƒn báº£n:** " + display_name
            else:
                header = "**Document:** " + display_name

            contents = []
            for section in sections:
                section_text = "\n".join(
                    self.collect_all_titles_and_texts_section(section))
                contents.append(section_text)

            block = header + "\n\n" + "\n\n".join(contents)
            all_blocks.append(block)

        # âœ… 3) Join blocks with ---
        full_text = "\n\n---\n\n".join(all_blocks)

        if need_agent:
            full_text += "\n\n[ThÃ´ng bÃ¡o: YÃªu cáº§u Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n chuyÃªn viÃªn tÆ° váº¥n.]"

        # âœ… 4) Clean markdown
        summary_answer = self.clean_markdown(summary_answer)
        full_text = self.clean_markdown(full_text)

        # âœ… 5) Äá»•i tÃªn **VÄƒn báº£n:** trong summary_answer thÃ nh filename thá»±c táº¿
        docname_to_filename = {}
        for (document, display_name), _ in doc_sections.items():
            docname_to_filename[document.strip()] = display_name
            docname_to_filename[display_name.strip()] = display_name

        def replace_doc_names(match):
            old_docname = match.group(1).strip()
            new_docname = docname_to_filename.get(old_docname, old_docname)
            print("Replacing docname:", old_docname, "â†’", new_docname)
            return f"**VÄƒn báº£n:** {new_docname}"

        summary_answer = re.sub(
            r"\*\*VÄƒn báº£n:\*\*\s*(.*?)\s*(?=\n|$)", replace_doc_names, summary_answer)

        # âœ… 6) Æ¯u tiÃªn sáº¯p xáº¿p cÃ¡c block theo QC â†’ TC â†’ khÃ¡c
        blocks = re.split(r'(?=\*\*VÄƒn báº£n:\*\*)', summary_answer)

        def get_priority(block):
            match = re.search(r"\*\*VÄƒn báº£n:\*\*\s*(.*?)\s*(?=\n|$)", block)
            if not match:
                return 3  # náº¿u khÃ´ng rÃµ thÃ¬ Æ°u tiÃªn tháº¥p nháº¥t
            name = match.group(1).strip().upper()
            if name.startswith("QCVN06"):
                return 0
            elif name.startswith("QC"):
                return 1
            elif name.startswith("TC"):
                return 2
            else:
                return 3

        blocks_sorted = sorted(blocks, key=get_priority)
        summary_answer = "\n".join(blocks_sorted).strip()

        return summary_answer, full_text

    def clean_markdown(self, text: str) -> str:
        # Replace 3+ newlines with 2
        text = re.sub(r'\n{3,}', '\n\n', text)
        # Remove leading/trailing newlines
        return text.strip()

    def collect_all_titles_and_texts_section(self, node):
        """
        Recursively collect:
        - the node's own section/subsection title (if any)
        - all text data inside
        - all nested sections/subsections titles too
        """
        texts = []

        # # âœ… 1) Include the current section/subsection title as text with leading \n
        title = node.get("section") or node.get("subsection")
        if title:
            texts.append("\n" + title)

        # âœ… 2) Include the current text node, if any, with \n if no '-' or '+' at start
        if node.get("type") == "text":
            data = node.get("data", "")
            if "|" in data:
                texts.append(data)
            elif data.lstrip().startswith(("-", "+")):
                texts.append(data)
            else:
                texts.append("\n" + data)
        elif node.get("type") == "image":
            image_md = node.get("image_markdown", "")
            image_desc = node.get("image_name", "Image")
            texts.append(f"![{image_desc}]({image_md})")

        # âœ… 3) Recurse into children
        for child in node.get("content", []):
            texts.extend(self.collect_all_titles_and_texts_section(child))

        return texts

    def collect_all_titles_and_texts(self, node, level=0):
        """
        Recursively collect:
        - Skip the top-level section/subsection title (level=0)
        - Include all nested section/subsection titles (level >= 1)
        - Collect all text and image nodes
        """
        texts = []

        # âœ… 1) Include title only if it's not the top-level one
        title = node.get("section") or node.get("subsection")
        if title and level > 0:
            texts.append("\n" + title)

        # âœ… 2) Include the current text node, if any
        if node.get("type") == "text":
            data = node.get("data", "")
            if "|" in data:
                texts.append(data)
            elif data.lstrip().startswith(("-", "+")):
                texts.append(data)
            else:
                texts.append("\n" + data)
        elif node.get("type") == "image":
            image_md = node.get("image_markdown", "")
            image_desc = node.get("image_name", "Image")
            texts.append(f"![{image_desc}]({image_md})")

        # âœ… 3) Recurse into children with increased level
        for child in node.get("content", []):
            texts.extend(self.collect_all_titles_and_texts(child, level=level+1))

        return texts

    def collect_all_content_with_headings(self, section, level=1):
        """
        Recursively collect all content with Markdown headings for sections/subsections.
        - Starts with level=1 (which means "#").
        - Each deeper subsection adds one more "#".
        """

        lines = []

        # Determine heading key: section or subsection
        heading = section.get("section") or section.get(
            "subsection") or "No Title"

        # Add heading with appropriate #
        heading_prefix = "#" * level
        lines.append(f"\n{heading}")

        # Add content (text or images)
        for item in section.get("content", []):
            # If this item is a nested subsection -> recurse
            if "subsection" in item:
                lines.extend(self.collect_all_content_with_headings(
                    item, level=level + 1))
            else:
                if item["type"] == "text":
                    lines.append(item["data"])
                elif item["type"] == "image":
                    image_md = item.get("image_markdown", "")
                    image_desc = item.get("image_name", "Image")
                    lines.append(f"![{image_desc}]({image_md})")

        return lines

    def search_bm25(self, bm25_plus, full_documents_bm25, query, text_n_results=2):
        bm25_embedding_fn = BM25EmbeddingFunction()
        query_tokens = bm25_embedding_fn.bm25_tokenizer(query)
        scores = bm25_plus.get_scores(query_tokens)

        top_indices = np.argsort(scores)[::-1][:text_n_results]
        top_results = [full_documents_bm25[i] for i in top_indices]

        # print("\n\nBM25 search results:\n")
        # for i, obj in enumerate(top_results):
        #     meta = obj.get("metadata", {})
        #     doc_name = meta.get("doc_name", "Unknown file")
        #     section_title = meta.get("section", "No Section Title")
        #     print(f"\n[BM25 chunk {i}] from file: {doc_name} - Title: {section_title} - Score: {scores[top_indices[i]]:.4f}")

        return top_results

    def parse_summary_answer_blocks(self, summary_text: str) -> List[Tuple[str, str, str]]:
        blocks = []
        current_doc_id = None
        current_title = None
        current_content_lines = []

        lines = summary_text.strip().splitlines()

        for line in lines:
            if line.startswith("**VÄƒn báº£n:**"):
                # Save previous block
                if current_doc_id is not None:
                    content = "\n".join(current_content_lines).strip()
                    blocks.append((current_doc_id, current_title, content))
                # Start new block
                current_doc_id = line.replace("**VÄƒn báº£n:**", "").strip()
                current_title = None
                current_content_lines = []
            elif "TiÃªu Ä‘á»:" in line:
                raw_title = line.split("TiÃªu Ä‘á»:", 1)[-1].strip()
                # Remove leading '**', '*', or extra spaces
                cleaned_title = re.sub(r"^\*+\s*", "", raw_title).strip()
                current_title = cleaned_title
                current_content_lines.append(line.strip())  # keep original for context
            else:
                current_content_lines.append(line.strip())

        # Save the last block
        if current_doc_id is not None:
            content = "\n".join(current_content_lines).strip()
            blocks.append((current_doc_id, current_title, content))

        return blocks
    
    def merge_answers(
        self,
        original_answer: dict,
        normalized_answer: dict,
        original_full_sections: List[str],
        normalized_full_sections: List[str]
    ) -> Tuple[dict, List[str]]:

        def parse_json_block(block):
            try:
                cleaned = block.strip().removeprefix("```json").removesuffix("```").strip()
                return json.loads(cleaned)
            except json.JSONDecodeError as e:
                print("âŒ JSON decode error:", e)
                print("ğŸ” Raw content that caused error:\n", block[:500])
                return {}

        # Step 1: Parse JSON
        original_json = parse_json_block(original_answer) if original_answer else {}
        normalized_json = parse_json_block(normalized_answer) if normalized_answer else {}

        # Step 2: Combine and deduplicate results
        seen = set()
        combined_results = []
        for result in original_json.get("results", []) + normalized_json.get("results", []):
            key = tuple(result)
            if key not in seen:
                seen.add(key)
                combined_results.append(result)

        # Step 3: Get (doc_id, title) from results
        relevant_keys = {
            tuple(result[:2]) for result in combined_results
            if isinstance(result, list) and len(result) >= 2
        }

        # Step 4: Parse summary blocks
        original_blocks = self.parse_summary_answer_blocks(original_json.get("summary_answer", ""))
        normalized_blocks = self.parse_summary_answer_blocks(normalized_json.get("summary_answer", ""))

        for doc_id, title, content in original_blocks + normalized_blocks:
            print(f"âœ… Parsed: {doc_id=} | {title=}")

        # Step 5: Merge summary blocks based on (doc_id, title)
        summary_blocks = []
        added_keys = set()

        ordered_results = original_json.get("results", []) + normalized_json.get("results", [])
        for result in ordered_results:
            if not isinstance(result, list) or len(result) < 2:
                continue
            doc_id, title = result[:2]
            key = (doc_id, title)
            if key in added_keys:
                continue
            added_keys.add(key)

            found = False
            for blocks in [original_blocks, normalized_blocks]:
                for block_doc_id, block_title, content in blocks:
                    if (block_doc_id, block_title) == key:
                        full_block = f"**VÄƒn báº£n:** {block_doc_id}\n{content}"
                        summary_blocks.append(full_block)
                        found = True
                        break
                if found:
                    break

        # Deduplicate summary blocks
        seen_summary_hashes = set()
        dedup_summary_blocks = []
        for blk in summary_blocks:
            blk_hash = hash(blk)
            if blk_hash not in seen_summary_hashes:
                seen_summary_hashes.add(blk_hash)
                dedup_summary_blocks.append(blk)

        summary_combined = "\n\n".join(dedup_summary_blocks).strip()

        # Step 6: Merge full_sections based on doc_id only
        all_sections = original_full_sections + normalized_full_sections
        merged_full_sections = []
        added_section_hashes = set()

        relevant_doc_ids = {doc_id for doc_id, _ in relevant_keys}

        for section in all_sections:
            lines = section.strip().splitlines()
            doc_id = None
            for line in lines:
                if line.lower().startswith("# vÄƒn báº£n:"):
                    doc_id = line.split(":", 1)[-1].strip()
                    break
            if doc_id and doc_id in relevant_doc_ids:
                section_hash = hash(section)
                if section_hash not in added_section_hashes:
                    added_section_hashes.add(section_hash)
                    merged_full_sections.append(section)

        # Step 7: Merge flags
        need_agent = original_json.get("needAgent", False) and normalized_json.get("needAgent", False)
        need_search = original_json.get("needSearch", False) and normalized_json.get("needSearch", False)

        merged_answer = {
            "summary_answer": summary_combined,
            "results": combined_results,
            "needAgent": need_agent,
            "needSearch": need_search,
        }

        return merged_answer, merged_full_sections

    def combined_answer(self, text_db, bm25_plus, full_documents_bm25, query, text_n_results=7, json_folder: str = "new-vn-data-json", language: str = "vi"):
        t0 = time.time()

        if language == "vi":
            normalized_query = self.normalize_viet(query)
        else:
            normalized_query = self.normalize_eng(query)

        print("ORIGINAL QUERY")
        t1 = time.time()
        # summary_answer, references = self.generate_answer(text_db, bm25_plus, full_documents_bm25, query)
        original_answer, original_full_sections = self.generate_answer(
            text_db, bm25_plus, full_documents_bm25, query, text_n_results=4)
        t2 = time.time()
        print(f"â±ï¸ Time to generate original_answer: {t2 - t1:.2f}s")

        print("NORMALIZED QUERY")
        # normalized_answer, normalized_references = self.generate_answer(text_db, bm25_plus, full_documents_bm25, normalized_query)
        t3 = time.time()
        normalized_answer, normalized_full_sections = self.generate_answer(
            text_db, bm25_plus, full_documents_bm25, normalized_query, text_n_results=4)
        t4 = time.time()
        print(f"â±ï¸ Time to generate normalized_answer: {t4 - t3:.2f}s")

        # prompt = self.voted_original_normalized_answer(query, original_answer, normalized_answer)
        # voted_answer = self.generate_model_answer(prompt)

        t5 = time.time()
        voted_answer, voted_full_sections = self.merge_answers(
            original_answer, normalized_answer, original_full_sections, normalized_full_sections)
        t6 = time.time()
        print(f"â±ï¸ Time to merge answers: {t6 - t5:.2f}s")

        print("\nVOTED_ANSWER", voted_answer)

        t7 = time.time()
        summary_answer, references = self.filter_answer(
            voted_full_sections, voted_answer, json_folder=json_folder, language=language)
        t8 = time.time()
        print(f"â±ï¸ Time to filter answer: {t8 - t7:.2f}s")

        total_time = time.time() - t0
        print(f"âœ… Total time for combined_answer: {total_time:.2f}s")

        return summary_answer, references

    def generate_answer(self, text_db, bm25_plus, full_documents_bm25, query, text_n_results=4, json_folder: str = "new-vn-data-json", language: str = "vi"):
        t0 = time.time()

        # segmented_question = query
        segmented_question = ViTokenizer.tokenize(query)
        text_res = text_db.query(query_texts=[segmented_question], n_results=text_n_results, include=[
                                 "documents", "metadatas", "distances"])
        t1 = time.time()
        print(f"â±ï¸ Vector DB query time: {t1 - t0:.2f}s")

        text_doc = text_res['documents'][0]
        text_metadata = text_res['metadatas'][0]
        text_distances = text_res['distances'][0]

        # Search for relevant sections in the BM25 database
        t2 = time.time()
        bm25_results = self.search_bm25(
            bm25_plus, full_documents_bm25, segmented_question, text_n_results=text_n_results)
        t3 = time.time()
        print(f"â±ï¸ BM25 search time: {t3 - t2:.2f}s")

        section_keys = set()
        full_sections = []

        # Trace back through the vector database results
        for i, (doc_text, meta, text_dis) in enumerate(zip(text_doc, text_metadata, text_distances)):
            doc_name = meta.get("doc_name", "Unknown file")
            section_title = meta.get("section", "No Section Title")
            key = (doc_name, section_title)

            if key not in section_keys:
                section_keys.add(key)
                # print(f"\n[Text chunk {i}] from file: {doc_name} - Title: {section_title} - Distance: {text_dis:.4f}")
                section_text = self.get_full_section_content(
                    doc_name, section_title)
                if language == "vi":
                    section_text = f"# VÄƒn báº£n: {doc_name}\n## TiÃªu Ä‘á»: {section_title}\n\nÄoáº¡n vÄƒn: {section_text}"
                else:
                    section_text = f"# Document: {doc_name}\n## Title: {section_title}\n\nText: {section_text}"

                if section_text:
                    full_sections.append(section_text)

        # Trace back through the BM25 results
        for i, obj in enumerate(bm25_results, text_n_results):
            meta = obj.get("metadata", {})
            doc_name = meta.get("doc_name", "Unknown file")
            section_title = meta.get("section", "No Section Title")
            key = (doc_name, section_title)

            if key not in section_keys:
                section_keys.add(key)
                bm25_text = obj.get("text", "")
                section_text = self.get_full_section_content(doc_name, section_title)
                if language == "vi":
                    section_text = f"# VÄƒn báº£n: {doc_name}\n## TiÃªu Ä‘á»: {section_title}\n\nÄoáº¡n vÄƒn:\n{section_text}"
                else:
                    section_text = f"# Document: {doc_name}\n## Title: {section_title}\n\nText: {section_text}"

                if section_text:
                    full_sections.append(section_text)

        # print(f"KONIS -- generate_answer -- full_sections {full_sections}")

        combined_context = "\n----\n".join(full_sections)

        if language == "vi":
            prompt = self.make_rag_prompt_viet(query, combined_context)
        else:
            prompt = self.make_rag_prompt_eng(query, combined_context)

        # Truncate the prompt to fit within the model's token limit
        if self.config.MODEL_USED == "openai":
            prompt = prompt[:200000]
        elif self.config.MODEL_USED == "gemini":
            prompt = prompt

        print(f"\n\nPrompt:\n{prompt}\n\n")

        t4 = time.time()
        answer = self.generate_model_answer(prompt)
        t5 = time.time()
        print(f"â±ï¸ Time to generate model answer: {t5 - t4:.2f}s")

        print(f"Query: {query}")
        print(f"Answer: {answer}")

        total_time = time.time() - t0
        print(f"âœ… Total time for generate_answer: {total_time:.2f}s")

        return answer, full_sections

    def generate_answer_with_source(self, text_db, image_db, table_db, query, text_n_results=25, image_n_results=3, table_n_results=3):

        query = self.normalize(query)

        text_res = text_db.query(query_texts=[query], n_results=text_n_results, include=[
                                 "documents", "metadatas", "distances"])
        image_res = image_db.query(query_texts=[query], n_results=image_n_results, include=[
                                   "documents", "metadatas", "distances"])
        table_res = table_db.query(query_texts=[query], n_results=table_n_results, include=[
                                   "documents", "metadatas", "distances"])
        text_doc = text_res['documents'][0]
        image_doc = image_res['documents'][0]
        table_doc = table_res['documents'][0]

        text_metadata = text_res['metadatas'][0]
        image_metadata = image_res['metadatas'][0]
        table_metadata = table_res['metadatas'][0]

        text_distances = text_res['distances'][0]
        image_distances = image_res['distances'][0]
        table_distances = table_res['distances'][0]

        prompt = self.make_rag_prompt(query, text_doc)
        answer = self.generate_model_answer(prompt)

        normalized = unicodedata.normalize("NFKC", answer or "").lower()

        if any(trigger in normalized for trigger in self.FALLBACK_TRIGGERS):
            print("ğŸ” Trigger fallback: No answer found in response.")
            answer = self.query_processor.search_and_answer(query)

        images_res = []
        tables_res = []

        for i, (doc_text, meta, text_dis) in enumerate(zip(text_doc, text_metadata, text_distances)):
            source_file = meta.get("filename", "Unknown file")
            print(
                f"\n[Text chunk {i}] from file: {source_file} - Distance: {text_dis}\nText:\n{doc_text}\n{'-'*40}")

        for i, (doc_text, meta, image_dis) in enumerate(zip(image_doc, image_metadata, image_distances)):
            source_file = meta.get("url", "Unknown file")
            if image_dis < 0.1:
                images_res.append(source_file)
            print(
                f"\n[Image chunk {i}] from file: {source_file} - Distance: {image_dis}\nText:\n{doc_text}\n{'-'*40}")

        for i, (doc_text, meta, table_dis) in enumerate(zip(table_doc, table_metadata, table_distances)):
            source_file = meta.get("url", "Unknown file")
            if table_dis < 0.1:
                tables_res.append(source_file)
            print(
                f"\n[Table chunk {i}] from file: {source_file} - Distance: {table_dis}\nText:\n{doc_text}\n{'-'*40}")

        answer += "\n\n"
        if images_res and tables_res:
            answer = "Tham kháº£o hÃ¬nh áº£nh vÃ  báº£ng sau:\n"
            # for img in images_res:
            #     answer += f"- HÃ¬nh áº£nh: {img}\n"
            # for table in tables_res:
            #     answer += f"- Báº£ng: {table}\n"
        elif images_res:
            answer = "Tham kháº£o hÃ¬nh áº£nh sau:\n"
            # for img in images_res:
            #     answer += f"- HÃ¬nh áº£nh: {img}\n"
        elif tables_res:
            answer = "Tham kháº£o báº£ng sau:\n"
            # for table in tables_res:
            #     answer += f"- Báº£ng: {table}\n"

        return (
            query,
            answer,
            images_res,
            tables_res,
            # Convert zip to list
            list(zip(text_doc, text_metadata, text_distances)),
            # Convert zip to list
            list(zip(image_doc, image_metadata, image_distances)),
            # Convert zip to list
            list(zip(table_doc, table_metadata, table_distances))
        )
